{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20838a8e-cf7f-47e0-8149-88c231d76763",
   "metadata": {},
   "source": [
    "# [10] CNN 돌리기 (CNN Activatioon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ab5fd8-ff2c-401b-8237-50c565c4f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR10 데이터 \n",
    "# 설명 : 10개 클래스  ,5만개 train image , 1만개 test image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dd44df-67da-4442-8112-8e248d32b189",
   "metadata": {},
   "source": [
    "# 맥락\n",
    "\n",
    "1. `Imports` 라이브러리\n",
    "2. `transforms data` 수행가능한 데이터 만들기(performing)\n",
    "3. `batch size` 세팅하기 train,test 셋 다운로드하기.\n",
    "4. validation 20% 로 대략설정하여 데이터를 sampler 로 나누기\n",
    "5. `DataLoader()` 함수로 각 세트의 배치정하기\n",
    "6. 구조 정의하기. (Define the architecture of your network)\n",
    "7. 파라미터정의하기.(epoch 도 정의)\n",
    "8. 실제로 모델 학습시키기. loss 와 accuracy 계산.\n",
    "9. Plot 으로 그래프그리기.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79664e4a-7fa9-402e-8e2e-a759a2a80555",
   "metadata": {},
   "source": [
    "## Data Augmentation (데이터증강)\n",
    "\n",
    "`-` 중요성 : 1. 숫자가 많으면 학습이 잘된다.(데이터부족해결, 일반화 성능향상, 과적합 방지) 그래서 여러 변형을 주며 데이터증강을 할 필요가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151eb992-c492-4acb-ad9f-b8b7db4e49d0",
   "metadata": {},
   "source": [
    "# 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "499246d6-af6d-423a-8afe-d61ed9274d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5e52089-0943-4351-a5f0-9e4ca110f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = \\\n",
    "transforms.Compose([transforms.ToTensor,\n",
    "                    transforms.Normalize((0.5,0.5,0.5),\n",
    "                                         (0.5,0.5,0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53e4bcd2-e3f8-44ee-b0da-94946ca5dff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "\n",
    "train_data =datasets.CIFAR10('data',train=True, \n",
    "                             download=True,\n",
    "                             transform=transform)\n",
    "test_data =datasets.CIFAR10('data',train=False, \n",
    "                             download=True,\n",
    "                             transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87928702-02ff-443a-9743-ab2b52e4e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_size = 0.2\n",
    "idx = list(range(len(train_data)))\n",
    "np.random.shuffle(idx)\n",
    "split_size = int(np.floor(dev_size * len(train_data)))\n",
    "train_idx,dev_idx = idx[split_size:],idx[:split_size]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx) #훈련셋 무작위 선택\n",
    "dev_sampler = SubsetRandomSampler(dev_idx)\n",
    "\n",
    "# 현재과정은 test 셋은 나두고 train셋을 8:2 로 나누어\n",
    "#dev 셋에서 학습 성능을 검증하기 위해 만든 세트입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cec2a610-c547-402d-aa0a-9a84f44c0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = \\\n",
    "torch.utils.data.DataLoader(train_data,\n",
    "                            batch_size=batch_size,\n",
    "                            sampler=train_sampler)\n",
    "dev_loader = \\\n",
    "torch.utils.data.DataLoader(train_data,\n",
    "                            batch_size=batch_size,\n",
    "                            sampler=dev_sampler)\n",
    "test_loader = \\\n",
    "torch.utils.data.DataLoader(test_data,\n",
    "                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4654b895-702f-401e-bad1-d80f66abf711",
   "metadata": {},
   "source": [
    "#이게 기초\n",
    "```python\n",
    "class CNN(nn.Module): #Pytorch 의 모든 신경망 모델의 상속 모델이다.\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # 모델의 레이어들을 여기에 정의\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 순전파 동작을 여기에 정의\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90261eef-e099-41d4-8115-d88e36fe338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # 모델의 레이어들을 여기에 정의\n",
    "        self.conv1 = nn.Conv2d(3,10,3,1,1)\n",
    "        self.conv2 = nn.Conv2d(10,20,3,1,1)\n",
    "        self.conv3 = nn.Conv2d(20,40,3,1,1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.linear1 = nn.Linear(40*4*4,100)\n",
    "        self.linear2 = nn.Linear(100,10)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 순전파 동작을 여기에 정의\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "\n",
    "        x = x.view(-1,40*4*4)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.log_softmax(self.linear2(x), dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9544a94-d03e-450e-9822-d744a2e9c110",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'NLLLose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m CNN()\n\u001b[0;32m----> 2\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNLLLose\u001b[49m() \u001b[38;5;66;03m#negative log-likelihood)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m      4\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'NLLLose'"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "loss_function = nn.NLLLose() #negative log-likelihood)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1a1fd9-4024-4054-88b5-68cee19e49af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
