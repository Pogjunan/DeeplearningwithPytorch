{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7705029-b7f1-43cf-a4dd-bbf9317aebc2",
   "metadata": {},
   "source": [
    "# `[3]` Build Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d5464f-c6da-45be-a99d-8a731a2607cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "대표적인 Neural Network 의 3가지 <br>\n",
    "`-` 1. Supervised learning <br>\n",
    "`-` 2. Unsupervised learning <br>\n",
    "`-` 3. Reinforcemnent learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f805a063-794f-4ec2-8b5a-f2e48e98654f",
   "metadata": {},
   "source": [
    "# `1` What are Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b934560-e1f6-4a12-bae7-a7064cc1d5c3",
   "metadata": {},
   "source": [
    "$$X_1 , .. X_4 -> perceptron -> output $$\n",
    "`-` $X_1 , ... X_4 $ 는 각자의 다른 input 을 의미한다. <br>\n",
    "`-` output = $ 0 $ or $ 1 $ (threshold 를 기준으로 w * x 로 결정) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83340db5-5de9-4acb-a9ad-779e5b07a1be",
   "metadata": {},
   "source": [
    "`1-1` Perceptron calculation 예시\n",
    "\n",
    "`[현재상황]` <br>\n",
    "오는 금요일에 뮤직페스티벌이 열리는데 지금 나는 몸이 아프다. 내가 뮤직페스티벌에 갈지안갈지 정하고있다.\n",
    "간다면 1 안간다면 0 으로 정한다. 이 때의 결정 요소로는\n",
    "- 1. 날씨가 좋은가? ($X_1$)\n",
    "- 2. 같이갈 누군가가 있는가? ($X_2$)\n",
    "- 3. 너가 좋아하는 음악인가? ($X_3$)\n",
    "<br>\n",
    "이 절차에 따라 진행하며 각각의 가중치가 다르게 적용될 것 이다. (날씨가 안좋아도 최대한 ok 이면 $w_1 = 0.1$ , 누군가랑 같이가는게 필수적이면 $w_2 = 0.9$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "`[인공지능의상황]`\n",
    "- 1. 날씨가 좋은가? ($X_1$)\n",
    "- 2. 같이갈 누군가가 있는가? ($X_2$)\n",
    "- 3. 너가 좋아하는 음악인가? ($X_3$)\n",
    "<br>\n",
    "\n",
    "perceptron 을 3개로 설정시에 이렇게될 수 있다.\n",
    "- 1번 perceptron : 날씨=0.3 , 같이갈사람 = 0.5 , 좋아하는 음악 = 0.7 \n",
    "- 2번 perceptron : 날씨 =0.9 , 같이갈사람 =0.3 , 좋아하는 음악 = 0.2\n",
    "- 3번 perceptron : 날씨 = 0.1 , 같이갈사람 = 0.9 , 좋아하는 음악 =0.8  \n",
    "\n",
    "이 숫자는 output 이 $0$ 또는 $1$ 이냐에 따라 역전파와 순전파가 반복되면서 가중치가 바뀌게 된다. (backpropagation , forward)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b569cc3-4150-4fbb-93ab-ae53bf96ee87",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `-` output 의 역할\n",
    "- output 은 2가지 역할을 합니다. \n",
    "1. ground truth(진짜 정답)과 비교하여 어떠한 결정을 하는 역할\n",
    "2. 수학적계산을 바탕으로 파라미터를 조정하는 역할\n",
    "\n",
    "<br>\n",
    "`2번`에 초점을 두자면 forward propagation(순전파) -> loss function -> backward propagation(역전파) 의 과정으로 가중치인 w 와 오차 등을 계산합니다.\n",
    "<br>\n",
    "그리고 이를 반복하여(iterative) 학습하는 것이 딥러닝의 계산방식입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6e57f7-a9ab-4c95-9399-13636e0f4d32",
   "metadata": {},
   "source": [
    "## `loss function` (손실함수)\n",
    "`-` 순전파가 종료되면 loss function 을 측정하여 에러를 추정하는 작업을 수행합니다. 0이 목적이되며 0 은 두 가지의 값이 같다는 것을 의미합니다(there is no divergence between the two values.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecfa9c9-b8ef-494f-8793-cd318b282a52",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` MSE (Mean squared error)\n",
    "<br>\n",
    "![](https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyMzA0MThfMTcz%2FMDAxNjgxNzk4ODYzNDQ5.hsj5hJxppOVZrEVNQmBFNm7twddAG4mbMWze0WGdQNEg.7x_blI0UJ6XwlhC-cgtKUPPrqDGWSxS8BiAs3kv541cg.PNG.lghmms%2Fimage.png&type=sc960_832)\n",
    "\n",
    "<br>\n",
    "`-` Cross-entropy/multi-class cross-entropy\n",
    "<br>\n",
    "$$ loss =- \\frac{1}{n}\\sum^n_{i=1} y_i * log(\\hat{y_i}) + ( 1- y_i) * log(1 - \\hat{y_i}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c0e369-1f8b-4746-ac09-a14c4cdc3e41",
   "metadata": {},
   "source": [
    "`-` 역전파 (간단)\n",
    "- training process 의 마지막 작업으로 `미분의 일부 작업`으로 loss function 로부터 $weights $ 와 $bias$ 를 얻어 파라미터를 업데이트합니다.\n",
    "그리고 다음 스텝으로 넘어갑니다.(so that in the next iteration step, the loss function is lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205eb245-424b-4f6c-81c6-f337c0c2b835",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` optimizer (간단)\n",
    "- 미분이 0 인 부분을 바로 찾으면 loss function 이 0인 가장 확실한 점이라고 생각할 수 있습니다.\n",
    "<br>\n",
    "그러나 미분이 0 이고 이차미분이 2x2 행렬인 경우 (1,1)에 위치한 값이 양수이고 이차미분의 행렬식이 양수인 경우 가장 loss function 이 작은 값이 됩니다.\n",
    "<br>\n",
    "딥러닝에는 파라미터가 몇 만개씩 됩니다. 이는 이차미분 행렬을 애초에 계산하기에 매우 어렵고 행렬식또한 계산하기 매우매우 어려움을 시사하고있습니다.\n",
    "<br>\n",
    "이에 대한 해결책으로 나온 방법이 `Gradient Descent` 입니다.\n",
    "\n",
    "### `Gradient Descent`(경사하강법)\n",
    "- 한국어로 보이듯 경사하강법처럼 산에서 내려가는 모습으로 움직입니다. 1번의 iteration 마다 loss 값이 줄어드는 것을 그래프에 표현한 것으로 이의원리는 step by step 으로 learning rate 를 지정해주고 가중치와 bias 의 값을 알고리즘에 따라 optimize(최적화)를 진행합니다.\n",
    "$$ w` = w - \\alpha *dw $$\n",
    "$$ b` = b - \\alpha * db$$\n",
    "\n",
    "`-` 여기서 $w` , b`$ 는 다음 iteration 을 의미합니다.\n",
    "`특히 경사하강법은 optimization 종류 중 하나로(가장 근본적인 알고리즘) 매우 많은 알고리즘이 개발되었고 기초를 경사하강법(gradient descent)에 두고있습니다.(sgd , adam 등 .. )`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2890c6d4-997f-49a3-82f6-36687ea9c5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hf)",
   "language": "python",
   "name": "hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
